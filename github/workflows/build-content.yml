name: Build posts metadata and RAG index

on:
  push:
    paths:
      - "posts/**/*.md"
      - "images/posts/**"
      - ".github/workflows/build-content.yml"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install python-frontmatter markdown-it-py beautifulsoup4 sentence-transformers

      - name: Generate Posts.json and posts_index.json
        run: |
          python - << 'PY'
          import json, re
          from pathlib import Path
          from datetime import datetime
          import frontmatter
          from markdown_it import MarkdownIt
          from bs4 import BeautifulSoup
          from sentence_transformers import SentenceTransformer

          ROOT = Path(".")
          POSTS_DIR = ROOT / "posts"
          OUT_POSTS = ROOT / "Posts.json"
          OUT_INDEX = ROOT / "posts_index.json"

          md = MarkdownIt()

          def file_title_from_name(name: str) -> str:
            m = re.match(r"(\d{4}-\d{2}-\d{2})-(.+)\.md$", name, re.I)
            base = m.group(2) if m else name.rsplit(".", 1)[0]
            base = base.replace("_", " ").replace("-", " ").strip()
            return base[:1].upper() + base[1:]

          def guess_date(name: str, meta_date: str|None) -> str:
            if meta_date:
              for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%d %H:%M:%S"):
                try:
                  return datetime.strptime(str(meta_date).split("Z")[0], fmt).strftime("%Y-%m-%d")
                except Exception:
                  pass
            m = re.match(r"(\d{4}-\d{2}-\d{2})-", name)
            if m:
              return m.group(1)
            return datetime.utcnow().strftime("%Y-%m-%d")

          def md_to_text(md_str: str) -> str:
            html = md.render(md_str)
            text = BeautifulSoup(html, "html.parser").get_text(" ")
            text = re.sub(r"\s+", " ", text).strip()
            return text

          def first_heading(md_str: str) -> str|None:
            for line in md_str.splitlines():
              if line.startswith("# "): return line[2:].strip()
              if line.startswith("## "): return line[3:].strip()
            return None

          def make_excerpt(md_str: str, n=180) -> str:
            txt = md_to_text(md_str)
            if len(txt) <= n: return txt
            cut = txt[:n].rsplit(" ", 1)[0]
            return cut + "â€¦"

          def find_images(md_str: str):
            imgs = []
            imgs += re.findall(r"!```math
[^```]*```KATEX_INLINE_OPEN([^)]+)KATEX_INLINE_CLOSE", md_str)
            imgs += re.findall(r'<img[^>]+src=["\']([^"\']+)["\']', md_str, flags=re.I)
            seen, out = set(), []
            for p in imgs:
              p = p.strip()
              if p and p not in seen:
                seen.add(p); out.append(p)
            return out

          def slurp_markdown_files():
            if not POSTS_DIR.exists():
              return []
            files = sorted(POSTS_DIR.rglob("*.md"))
            items = []
            for path in files:
              parts = [p.lower() for p in path.parts]
              if any(p in {".drafts", "_drafts", ".trash"} for p in parts):
                continue
              post = frontmatter.load(path)
              meta = post.metadata or {}
              content = post.content or ""
              name = path.name

              title = str(meta.get("title") or first_heading(content) or file_title_from_name(name)).strip()
              date  = guess_date(name, meta.get("date"))
              ptype = str(meta.get("type", "general")).strip().lower()
              excerpt = str(meta.get("excerpt") or make_excerpt(content)).strip()
              img_list = meta.get("images")
              if not isinstance(img_list, list):
                img_list = find_images(content)
              cover = meta.get("cover") or (img_list[0] if img_list else "")

              rel = path.as_posix()
              items.append({
                "title": title,
                "date": date,
                "type": ptype,
                "excerpt": excerpt,
                "file": rel,
                "cover": cover,
                "images": img_list
              })
            def sort_key(x):
              try: return datetime.strptime(x["date"], "%Y-%m-%d")
              except: return datetime.min
            items.sort(key=sort_key, reverse=True)
            return items

          posts = slurp_markdown_files()

          OUT_POSTS.write_text(json.dumps(posts, ensure_ascii=False, indent=2), encoding="utf-8")
          print(f"Wrote {OUT_POSTS} with {len(posts)} posts")

          chunks_out = []
          if posts:
            model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", device="cpu")
            def chunk_text(text, size=900, overlap=220):
              out, i = [], 0
              while i < len(text):
                out.append(text[i:i+size])
                i += size - overlap
              return [c.strip() for c in out if c.strip()]

            for p in posts:
              md_path = ROOT / p["file"]
              if not md_path.exists():
                print("Missing file:", p["file"]); 
                continue
              raw = md_path.read_text(encoding="utf-8", errors="ignore")
              plain = md_to_text(raw)
              for j, ck in enumerate(chunk_text(plain, 900, 220)):
                emb = model.encode([ck], normalize_embeddings=True)[0].tolist()
                url = f"post.html?file={p['file']}"
                chunks_out.append({
                  "id": f"{p['file']}#{j}",
                  "title": p["title"],
                  "url": url,
                  "date": p["date"],
                  "text": ck,
                  "vec": emb
                })

          OUT_INDEX.write_text(json.dumps({"model":"all-MiniLM-L6-v2","dim":384,"chunks":chunks_out}, ensure_ascii=False), encoding="utf-8")
          print(f"Wrote {OUT_INDEX} with {len(chunks_out)} chunks")
          PY

      - name: Commit outputs
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore: auto-build Posts.json and posts_index.json"
          file_pattern: |
            Posts.json
            posts_index.json
